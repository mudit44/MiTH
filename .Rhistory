rm(list = ls(all=TRUE))
#read the data
train <- read.csv('train.csv',header = TRUE, na.strings = c(' ','','Unknown','NA','-','NA ',' NA'))
train_labels <- read.csv('trainlabels.csv', header = TRUE, na.strings = c(' ','','Unknown','NA','-'))
test <- read.csv('test.csv', header = TRUE, na.strings = c(' ','','Unknown','NA','-','NA ',' NA'))
#merge two dataframes
data <- merge(train,train_labels,by="Id")
#change column name from id to Id
colnames(train_labels)[which(names(train_labels) == "id")] <- "Id"
#merge two dataframes
data <- merge(train,train_labels,by="Id")
#if found any 0 values impute them with NA
data$Organization_funding[data$Organization_funding == 0] <- NA
data$Company_installed[data$Company_installed == 0] <- NA
#factor variable
data$District_code[data$District_code == 0] <- NA
write.csv(data,'visualisation.csv',row.names = FALSE)
rm(list = ls(all=TRUE))
#read the data
train <- read.csv('train.csv',header = TRUE, na.strings = c(' ','','Unknown','NA','-','NA ',' NA'))
train_labels <- read.csv('trainlabels.csv', header = TRUE, na.strings = c(' ','','Unknown','NA','-'))
test <- read.csv('test.csv', header = TRUE, na.strings = c(' ','','Unknown','NA','-','NA ',' NA'))
#public meeting and permit are to be converted to 1 or 0(logical)
train$Public_meeting <- ifelse(train$Public_meeting == "TRUE", 1, 0)
train$Permit <- ifelse(train$Permit == "TRUE", 1, 0)
train$Public_meeting <- as.logical(train$Public_meeting)
train$Permit <- as.logical(train$Permi)
test$Public_meeting <- ifelse(test$Public_meeting == "TRUE", 1,0)
test$Permit <- ifelse(test$Permit == "TRUE",1,0)
test$Public_meeting <- as.logical(test$Public_meeting)
test$Permit <- as.logical(test$Permit)
#change column name from id to Id
colnames(train_labels)[which(names(train_labels) == "id")] <- "Id"
#merge two dataframes
data <- merge(train,train_labels,by="Id")
data$Permit <- ifelse(test$Permit == "TRUE",1,0)
data$Status <- as.factor(data$Status)
rm('train','train_labels')
rm(list = ls(all=TRUE))
#read the data
train <- read.csv('train.csv',header = TRUE, na.strings = c(' ','','Unknown','NA','-','NA ',' NA'))
train_labels <- read.csv('trainlabels.csv', header = TRUE, na.strings = c(' ','','Unknown','NA','-'))
test <- read.csv('test.csv', header = TRUE, na.strings = c(' ','','Unknown','NA','-','NA ',' NA'))
#public meeting and permit are to be converted to 1 or 0(logical)
train$Public_meeting <- ifelse(train$Public_meeting == "TRUE", 1, 0)
train$Permit <- ifelse(train$Permit == "TRUE", 1, 0)
train$Public_meeting <- as.logical(train$Public_meeting)
train$Permit <- as.logical(train$Permi)
test$Public_meeting <- ifelse(test$Public_meeting == "TRUE", 1,0)
test$Permit <- ifelse(test$Permit == "TRUE",1,0)
test$Public_meeting <- as.logical(test$Public_meeting)
test$Permit <- as.logical(test$Permit)
###################################################################################
#Merge train and train_labels
###################################################################################
#change column name from id to Id
colnames(train_labels)[which(names(train_labels) == "id")] <- "Id"
#merge two dataframes
data <- merge(train,train_labels,by="Id")
data$Permit <- ifelse(test$Permit == "TRUE",1,0)
data$Status <- as.factor(data$Status)
###################################################################################
#EDA
###################################################################################
library(funModeling)
df_status(data)
df_status(test)
rm('train','train_labels')
#from observations and lib(fun_modelling) of heatmap SchemeName has highest number of missing values
#both in train and test dataset, remove them rather than imputing
data$SchemeName <- NULL
test$SchemeName <- NULL
#check for near zero variance
library(caret)
sapply(data, function(x) nearZeroVar(x))
sapply(test, function(x) nearZeroVar(x))
#Organization_surveyed is a constant value and can be dropped
data$Organization_surveyed <- NULL
test$Organization_surveyed <- NULL
#Gps_height can be be altitude from sea level so it can accomodate
#values ranging from -inf to +inf
nearZeroVar(data$Gps_height) #this is showing one
nearZeroVar(test$Gps_height) #this is showing zero
#needs further checking
table(sign(data$Gps_height)) #to check ratio of +ve and -ve values
table(sign(test$Gps_height)) #to check ratio of +ve and -ve values
#population column in both train and test
hist(data$Population, breaks = 5000)
#bin the data
summary(data$Population)
summary(test$Population)
#these points can be subsetted and can be analysed later if necessary
table(data$Population > 4000)
table(test$Population > 4000)
#after seeing the summary remove the outlier
hist(data$Population,breaks = 100)
hist(test$Population, breaks = 100)
#subset the observations which have population greater than 4000
#population_above_4000_data <- subset(data,train$Population > 4000)
#population_above_4000_test <- subset(test,test$Population > 4000)
#remove them from train and test
#data <- subset(data,data$Population < 4000)
#test <- subset(test, test$Population < 4000)
#new region code that is to be removed
which(data$Region_code == 40) #row number 3685
data <- data[-c(3685),]
#new scheme management that is to be removed
which(data$Scheme_management == "None")
data <- data[-c(14181),]
#checking Organization_funding
summary(data$Organization_funding)
df_status(data$Organization_funding)
#will check this later on
#quantity and quantity_group are same
table(data$Quantity)
table(data$Quantity_group)
#drop Quantity group from train and test both
data$Quantity_group <- NULL
test$Quantity_group <- NULL
#source and source_type are similar
table(data$Source)
table(data$Source_class)
table(data$Source_type)
#Source_type is generalising so can be dropped
data$Source_type <- NULL
test$Source_type <- NULL
#remove train and train_labels from the main memory
#rm('train_labels')
###################################################################################
#Data preparation
###################################################################################
#drop id column from data and test
data$Id <- NULL
test_id <- test$Id #store Id for submission purpose
test$Id <- NULL
#check for 0 values in factor variables
df_status(data)
#if found any 0 values impute them with NA
data$Organization_funding[data$Organization_funding == 0] <- NA
data$Company_installed[data$Company_installed == 0] <- NA
test$Organization_funding[test$Organization_funding == 0] <- NA
test$Company_installed[test$Company_installed == 0] <- NA
summary(data$Amount_of_water)
table(data$Amount_of_water < 3000)
# #new categorization based on water level
# data$water_level <- 0
# test$water_level <- 0
#
# #for data
# for(i in 1:nrow(data)){
#   if(data$Amount_of_water[i] >= 10000){
#     data$water_level[i] = 2
#   }
#   else if((data$Amount_of_water[i] < 10000) && (data$Amount_of_water[i] > 3000)){
#     data$water_level[i] = 1
#   }
#   else{
#     data$water_level[i] = 0
#   }
# }
#
# #for test
# for(i in 1:nrow(test)){
#   if(test$Amount_of_water[i] >= 10000){
#     test$water_level[i] = 2
#   }
#   else if((test$Amount_of_water[i] < 10000) && (test$Amount_of_water[i] > 3000)){
#     test$water_level[i] = 1
#   }
#   else{
#     test$water_level[i] = 0
#   }
# }
# Majority or (98%) were 0 values
df_status(data)
#factor variable
data$District_code[data$District_code == 0] <- NA
test$District_code[test$District_code == 0] <- NA
#village and water_point_name have too many unique values which can be thought
#as id
#remove it from data and test
data$Village <- NULL
data$Waterpointname <- NULL
test$Village <- NULL
test$Waterpointname <- NULL
#change region code and district code features to factor
#although they are integers but they a small no. of unique values
data$Region_code <- as.factor(data$Region_code)
data$District_code <- as.factor(data$District_code)
test$Region_code <- as.factor(test$Region_code)
test$District_code <- as.factor(test$District_code)
#as it has near zero variance
boxplot(data$Gps_height)
#new dataset for data visualisation
#write.csv(data,'visualisation.csv',row.names = FALSE)
###################################################################################
#Train Test Split
###################################################################################
rows <- seq(1,nrow(data),1)
set.seed(44)
train_rows <- sample(rows,(70*nrow(data))/100)
train <- data[train_rows,]
validation <- data[-train_rows,]
###################################################################################
#Imputation
###################################################################################
library(DMwR)
train <- centralImputation(train)
df_status(validation)
#impute with median/mode
# Public_meeting
# Organization_funding
# Scheme_management
# Permit
# Company_installed
#mode function
#finding mode function
get_mode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
validation$Organization_funding[is.na(validation$Organization_funding)] <- get_mode(train$Organization_funding)
validation$Scheme_management[is.na(validation$Scheme_management)] <- get_mode(train$Scheme_management)
validation$Company_installed[is.na(validation$Company_installed)] <- get_mode(train$Company_installed)
validation$Public_meeting[is.na(validation$Public_meeting)] <- get_mode(train$Public_meeting)
validation$Permit[is.na(validation$Permit)] <- get_mode(train$Permit)
#removing some more useless variables as system showing cannot allocate
#2.1 GB memory for a model
train$Wardname <- NULL
train$Organization_funding <- NULL
train$Company_installed <- NULL
data$Wardname <- NULL
data$Organization_funding <- NULL
data$Company_installed <- NULL
test$Wardname <- NULL
test$Organization_funding <- NULL
test$Company_installed <- NULL
validation$Wardname <- NULL
validation$Organization_funding <- NULL
validation$Company_installed <- NULL
data$Extraction_type_group <- NULL
train$Extraction_type_group <- NULL
validation$Extraction_type_group <- NULL
test$Extraction_type_group <- NULL
table(data$Scheme_management)
table(test$Scheme_management)
rm(list = ls(all=TRUE))
#read the data
train <- read.csv('train.csv',header = TRUE, na.strings = c(' ','','Unknown','NA','-','NA ',' NA'))
train_labels <- read.csv('trainlabels.csv', header = TRUE, na.strings = c(' ','','Unknown','NA','-'))
test <- read.csv('test.csv', header = TRUE, na.strings = c(' ','','Unknown','NA','-','NA ',' NA'))
#public meeting and permit are to be converted to 1 or 0(logical)
train$Public_meeting <- ifelse(train$Public_meeting == "TRUE", 1, 0)
train$Permit <- ifelse(train$Permit == "TRUE", 1, 0)
train$Public_meeting <- as.logical(train$Public_meeting)
train$Permit <- as.logical(train$Permi)
test$Public_meeting <- ifelse(test$Public_meeting == "TRUE", 1,0)
test$Permit <- ifelse(test$Permit == "TRUE",1,0)
test$Public_meeting <- as.logical(test$Public_meeting)
test$Permit <- as.logical(test$Permit)
###################################################################################
#Merge train and train_labels
###################################################################################
#change column name from id to Id
colnames(train_labels)[which(names(train_labels) == "id")] <- "Id"
#merge two dataframes
data <- merge(train,train_labels,by="Id")
data$Permit <- ifelse(test$Permit == "TRUE",1,0)
data$Status <- as.factor(data$Status)
###################################################################################
#EDA
###################################################################################
library(funModeling)
df_status(data)
df_status(test)
rm('train','train_labels')
#from observations and lib(fun_modelling) of heatmap SchemeName has highest number of missing values
#both in train and test dataset, remove them rather than imputing
data$SchemeName <- NULL
test$SchemeName <- NULL
#check for near zero variance
library(caret)
sapply(data, function(x) nearZeroVar(x))
sapply(test, function(x) nearZeroVar(x))
#Organization_surveyed is a constant value and can be dropped
data$Organization_surveyed <- NULL
test$Organization_surveyed <- NULL
#Gps_height can be be altitude from sea level so it can accomodate
#values ranging from -inf to +inf
nearZeroVar(data$Gps_height) #this is showing one
nearZeroVar(test$Gps_height) #this is showing zero
#needs further checking
table(sign(data$Gps_height)) #to check ratio of +ve and -ve values
table(sign(test$Gps_height)) #to check ratio of +ve and -ve values
#population column in both train and test
hist(data$Population, breaks = 5000)
#bin the data
summary(data$Population)
summary(test$Population)
#these points can be subsetted and can be analysed later if necessary
table(data$Population > 4000)
table(test$Population > 4000)
#after seeing the summary remove the outlier
hist(data$Population,breaks = 100)
hist(test$Population, breaks = 100)
#subset the observations which have population greater than 4000
#population_above_4000_data <- subset(data,train$Population > 4000)
#population_above_4000_test <- subset(test,test$Population > 4000)
#remove them from train and test
#data <- subset(data,data$Population < 4000)
#test <- subset(test, test$Population < 4000)
#new region code that is to be removed
which(data$Region_code == 40) #row number 3685
data <- data[-c(3685),]
#new scheme management that is to be removed
which(data$Scheme_management == "None")
data <- data[-c(14181),]
#checking Organization_funding
summary(data$Organization_funding)
df_status(data$Organization_funding)
#will check this later on
#quantity and quantity_group are same
table(data$Quantity)
table(data$Quantity_group)
#drop Quantity group from train and test both
data$Quantity_group <- NULL
test$Quantity_group <- NULL
#source and source_type are similar
table(data$Source)
table(data$Source_class)
table(data$Source_type)
#Source_type is generalising so can be dropped
data$Source_type <- NULL
test$Source_type <- NULL
#remove train and train_labels from the main memory
#rm('train_labels')
###################################################################################
#Data preparation
###################################################################################
#drop id column from data and test
data$Id <- NULL
test_id <- test$Id #store Id for submission purpose
test$Id <- NULL
#check for 0 values in factor variables
df_status(data)
#if found any 0 values impute them with NA
data$Organization_funding[data$Organization_funding == 0] <- NA
data$Company_installed[data$Company_installed == 0] <- NA
test$Organization_funding[test$Organization_funding == 0] <- NA
test$Company_installed[test$Company_installed == 0] <- NA
summary(data$Amount_of_water)
table(data$Amount_of_water < 3000)
# #new categorization based on water level
# data$water_level <- 0
# test$water_level <- 0
#
# #for data
# for(i in 1:nrow(data)){
#   if(data$Amount_of_water[i] >= 10000){
#     data$water_level[i] = 2
#   }
#   else if((data$Amount_of_water[i] < 10000) && (data$Amount_of_water[i] > 3000)){
#     data$water_level[i] = 1
#   }
#   else{
#     data$water_level[i] = 0
#   }
# }
#
# #for test
# for(i in 1:nrow(test)){
#   if(test$Amount_of_water[i] >= 10000){
#     test$water_level[i] = 2
#   }
#   else if((test$Amount_of_water[i] < 10000) && (test$Amount_of_water[i] > 3000)){
#     test$water_level[i] = 1
#   }
#   else{
#     test$water_level[i] = 0
#   }
# }
# Majority or (98%) were 0 values
df_status(data)
#factor variable
data$District_code[data$District_code == 0] <- NA
test$District_code[test$District_code == 0] <- NA
#village and water_point_name have too many unique values which can be thought
#as id
#remove it from data and test
data$Village <- NULL
data$Waterpointname <- NULL
test$Village <- NULL
test$Waterpointname <- NULL
#change region code and district code features to factor
#although they are integers but they a small no. of unique values
data$Region_code <- as.factor(data$Region_code)
data$District_code <- as.factor(data$District_code)
test$Region_code <- as.factor(test$Region_code)
test$District_code <- as.factor(test$District_code)
#as it has near zero variance
boxplot(data$Gps_height)
#new dataset for data visualisation
#write.csv(data,'visualisation.csv',row.names = FALSE)
###################################################################################
#Train Test Split
###################################################################################
rows <- seq(1,nrow(data),1)
set.seed(44)
train_rows <- sample(rows,(70*nrow(data))/100)
train <- data[train_rows,]
validation <- data[-train_rows,]
###################################################################################
#Imputation
###################################################################################
library(DMwR)
train <- centralImputation(train)
df_status(validation)
#impute with median/mode
# Public_meeting
# Organization_funding
# Scheme_management
# Permit
# Company_installed
#mode function
#finding mode function
get_mode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
validation$Organization_funding[is.na(validation$Organization_funding)] <- get_mode(train$Organization_funding)
validation$Scheme_management[is.na(validation$Scheme_management)] <- get_mode(train$Scheme_management)
validation$Company_installed[is.na(validation$Company_installed)] <- get_mode(train$Company_installed)
validation$Public_meeting[is.na(validation$Public_meeting)] <- get_mode(train$Public_meeting)
validation$Permit[is.na(validation$Permit)] <- get_mode(train$Permit)
#removing some more useless variables as system showing cannot allocate
#2.1 GB memory for a model
train$Wardname <- NULL
train$Organization_funding <- NULL
train$Company_installed <- NULL
data$Wardname <- NULL
data$Organization_funding <- NULL
data$Company_installed <- NULL
test$Wardname <- NULL
test$Organization_funding <- NULL
test$Company_installed <- NULL
validation$Wardname <- NULL
validation$Organization_funding <- NULL
validation$Company_installed <- NULL
data$Extraction_type_group <- NULL
train$Extraction_type_group <- NULL
validation$Extraction_type_group <- NULL
test$Extraction_type_group <- NULL
table(data$Scheme_management)
table(test$Scheme_management)
######################################################################################
#Base Model Building
######################################################################################
library(e1071)
naivebayes_model <- naiveBayes(Status~., data = train)
naivebayes_model
#predicitons for train
predictions_train <- predict(naivebayes_model, train)
table(predictions_train, train$Status)
confusion_matrix <- table(train$Status, predictions_train)
print(confusion_matrix)
specificity <- confusion_matrix[1, 1]/sum(confusion_matrix[1, ])
sensitivity <- confusion_matrix[2, 2]/sum(confusion_matrix[2, ])
accuracy <- sum(diag(confusion_matrix))/sum(confusion_matrix)
specificity
sensitivity
accuracy
#predictions for validation
predictions_validation <- predict(naivebayes_model, validation)
table(predictions_validation, validation$Status)
confusion_matrix <- table(validation$Status, predictions_validation)
print(confusion_matrix)
specificity <- confusion_matrix[1, 1]/sum(confusion_matrix[1, ])
sensitivity <- confusion_matrix[2, 2]/sum(confusion_matrix[2, ])
accuracy <- sum(diag(confusion_matrix))/sum(confusion_matrix)
specificity
sensitivity
accuracy
